{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNaIzSyo/QbVEH1oyoLCSOU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mikesheard/nanochat/blob/main/Nanochat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/karpathy/nanochat/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMAbHJOXGolM",
        "outputId": "eb8b8cec-9a64-4554-9298-7d54f5294888"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'nanochat' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd nanochat/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duRjB_TCBRm3",
        "outputId": "6cd1ba8e-b9fc-430e-b664-664bea8ba5af"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nanochat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fab5fe8f",
        "outputId": "cc960fae-20ea-4020-cb48-6b0e2839e87d"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Oct 16 13:07:09 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be4d2f04",
        "outputId": "e8fa860a-4c55-4f1c-d439-611c167fd9fd"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA is available!\")\n",
        "else:\n",
        "    print(\"CUDA is not available.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"GPU count:\", torch.cuda.device_count())\n",
        "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AusHr0xMQIof",
        "outputId": "997788be-125f-49f5-84c1-bf9ff53b2ad9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "GPU count: 1\n",
            "GPU name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRD4I8RP__qV",
        "outputId": "3c275a4f-2594-4cec-d8d0-564619609386"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe `--system` flag has no effect, a system Python interpreter is always used in `uv venv`\u001b[0m\n",
            "Using CPython 3.10.12 interpreter at: \u001b[36m/usr/bin/python3.10\u001b[39m\n",
            "Creating virtual environment at: \u001b[36m.venv\u001b[39m\n",
            "Activate with: \u001b[32msource .venv/bin/activate\u001b[39m\n",
            "\u001b[2mResolved \u001b[1m88 packages\u001b[0m \u001b[2min 19ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m86 packages\u001b[0m \u001b[2min 2m 16s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m86 packages\u001b[0m \u001b[2min 829ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1maiohappyeyeballs\u001b[0m\u001b[2m==2.6.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1maiohttp\u001b[0m\u001b[2m==3.12.15\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1maiosignal\u001b[0m\u001b[2m==1.4.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mannotated-types\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1manyio\u001b[0m\u001b[2m==4.10.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1masync-timeout\u001b[0m\u001b[2m==5.0.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mattrs\u001b[0m\u001b[2m==25.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcertifi\u001b[0m\u001b[2m==2025.8.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcharset-normalizer\u001b[0m\u001b[2m==3.4.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.2.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdatasets\u001b[0m\u001b[2m==4.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdill\u001b[0m\u001b[2m==0.3.8\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mexceptiongroup\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfastapi\u001b[0m\u001b[2m==0.117.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.19.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfiles-to-prompt\u001b[0m\u001b[2m==0.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfrozenlist\u001b[0m\u001b[2m==1.7.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgitdb\u001b[0m\u001b[2m==4.0.12\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgitpython\u001b[0m\u001b[2m==3.1.45\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mh11\u001b[0m\u001b[2m==0.16.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhf-xet\u001b[0m\u001b[2m==1.1.9\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.34.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1midna\u001b[0m\u001b[2m==3.10\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1miniconfig\u001b[0m\u001b[2m==2.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjinja2\u001b[0m\u001b[2m==3.1.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==3.0.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmaturin\u001b[0m\u001b[2m==1.9.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmpmath\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmultidict\u001b[0m\u001b[2m==6.6.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmultiprocess\u001b[0m\u001b[2m==0.70.16\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnanochat\u001b[0m\u001b[2m==0.1.0 (from file:///content/nanochat)\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.4.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.8.4.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.8.93\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.10.2.21\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.3.3.83\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufile-cu12\u001b[0m\u001b[2m==1.13.1.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.9.90\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.7.3.90\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.8.93\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.7.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.27.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.8.93\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpackaging\u001b[0m\u001b[2m==25.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==2.3.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mplatformdirs\u001b[0m\u001b[2m==4.4.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpluggy\u001b[0m\u001b[2m==1.6.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpropcache\u001b[0m\u001b[2m==0.3.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==6.32.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpsutil\u001b[0m\u001b[2m==7.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyarrow\u001b[0m\u001b[2m==21.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.11.7\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpydantic-core\u001b[0m\u001b[2m==2.33.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpygments\u001b[0m\u001b[2m==2.19.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpytest\u001b[0m\u001b[2m==8.4.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpython-dateutil\u001b[0m\u001b[2m==2.9.0.post0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpytz\u001b[0m\u001b[2m==2025.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyyaml\u001b[0m\u001b[2m==6.0.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mregex\u001b[0m\u001b[2m==2025.9.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msentry-sdk\u001b[0m\u001b[2m==2.35.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msetuptools\u001b[0m\u001b[2m==80.9.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msix\u001b[0m\u001b[2m==1.17.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msmmap\u001b[0m\u001b[2m==5.0.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msniffio\u001b[0m\u001b[2m==1.3.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mstarlette\u001b[0m\u001b[2m==0.48.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.14.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtiktoken\u001b[0m\u001b[2m==0.11.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.22.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtomli\u001b[0m\u001b[2m==2.2.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.8.0+cu128\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.67.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.4.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.15.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtyping-inspection\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtzdata\u001b[0m\u001b[2m==2025.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.5.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1muvicorn\u001b[0m\u001b[2m==0.36.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mwandb\u001b[0m\u001b[2m==0.21.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mxxhash\u001b[0m\u001b[2m==3.5.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1myarl\u001b[0m\u001b[2m==1.20.1\u001b[0m\n",
            "Reset report and wrote header to /root/.cache/nanochat/report/header.md\n",
            "\u001b[1minfo:\u001b[0m downloading installer\n",
            "\u001b[0m\u001b[1m\u001b[33mwarn: \u001b[0mIt looks like you have an existing rustup settings file at:\n",
            "\u001b[0m\u001b[1m\u001b[33mwarn: \u001b[0m/root/.rustup/settings.toml\n",
            "\u001b[0m\u001b[1m\u001b[33mwarn: \u001b[0mRustup will install the default toolchain as specified in the settings file,\n",
            "\u001b[0m\u001b[1m\u001b[33mwarn: \u001b[0minstead of the one inferred from the default host triple.\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mprofile set to 'default'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdefault host triple is x86_64-unknown-linux-gnu\n",
            "\u001b[0m\u001b[1m\u001b[33mwarn: \u001b[0mUpdating existing toolchain, profile choice will be ignored\n",
            "\u001b[0m\u001b[1minfo: \u001b[0msyncing channel updates for 'stable-x86_64-unknown-linux-gnu'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdefault toolchain set to 'stable-x86_64-unknown-linux-gnu'\n",
            "\n",
            "  \u001b[0m\u001b[1mstable-x86_64-unknown-linux-gnu unchanged\u001b[0m - rustc 1.90.0 (1159e78c4 2025-09-14)\n",
            "\n",
            "\u001b[0m\u001b[1m\n",
            "Rust is installed now. Great!\n",
            "\u001b[0m\n",
            "To get started you may need to restart your current shell.\n",
            "This would reload your \u001b[0m\u001b[1mPATH\u001b[0m environment variable to include\n",
            "Cargo's bin directory ($HOME/.cargo/bin).\n",
            "\n",
            "To configure your current shell, you need to source\n",
            "the corresponding \u001b[0m\u001b[1menv\u001b[0m file under $HOME/.cargo.\n",
            "\n",
            "This is usually done by running one of the following (note the leading DOT):\n",
            ". \"$HOME/.cargo/env\"            # For sh/bash/zsh/ash/dash/pdksh\n",
            "source \"$HOME/.cargo/env.fish\"  # For fish\n",
            "source $\"($nu.home-path)/.cargo/env.nu\"  # For nushell\n",
            "\u001b[1m\u001b[32m    Updating\u001b[0m crates.io index\n",
            "\u001b[1m\u001b[32m Downloading\u001b[0m crates ...\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m autocfg v1.5.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m cfg-if v1.0.3\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m bit-set v0.8.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m bit-vec v0.8.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m unindent v0.2.4\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m either v1.15.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m memoffset v0.9.1\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m version_check v0.9.5\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m unicode-ident v1.0.18\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m static_assertions v1.1.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m zerocopy-derive v0.8.26\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m pyo3-log v0.12.4\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m portable-atomic v1.11.1\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m memchr v2.7.5\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m syn v2.0.106\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m fancy-regex v0.16.1\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m indexmap v2.11.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m zerocopy v0.8.26\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m hashbrown v0.15.5\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m rayon-core v1.13.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m libc v0.2.175\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m wit-bindgen v0.45.1\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m rayon v1.11.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m once_cell v1.21.3\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m regex-syntax v0.8.6\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m crossbeam-utils v0.8.21\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m regex-automata v0.4.10\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m pyo3-ffi v0.23.5\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m pyo3-macros-backend v0.23.5\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m proc-macro2 v1.0.101\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m wasi v0.14.4+wasi-0.2.4\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m log v0.4.28\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m dary_heap v0.3.7\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m rustversion v1.0.22\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m quote v1.0.40\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m pyo3-macros v0.23.5\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m pyo3-build-config v0.23.5\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m itoa v1.0.15\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m heck v0.5.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m equivalent v1.0.2\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m crossbeam-epoch v0.9.18\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m crossbeam-deque v0.8.6\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m target-lexicon v0.12.16\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m ryu v1.0.20\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m indoc v2.0.6\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m r-efi v5.3.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m getrandom v0.3.3\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m compact_str v0.9.0\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m aho-corasick v1.1.3\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m ahash v0.8.12\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m arc-swap v1.7.1\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m castaway v0.2.4\n",
            "\u001b[1m\u001b[32m  Downloaded\u001b[0m pyo3 v0.23.5\n",
            "üîó Found pyo3 bindings\n",
            "üêç Found CPython 3.10 at /content/nanochat/.venv/bin/python\n",
            "üì° Using build options bindings from pyproject.toml\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m77 packages\u001b[0m \u001b[2min 833ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 545ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m3 packages\u001b[0m \u001b[2min 42ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m4 packages\u001b[0m \u001b[2min 16ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfiles-to-prompt\u001b[0m\u001b[2m==0.6\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.0.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mpsutil\u001b[0m\u001b[2m==5.9.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpsutil\u001b[0m\u001b[2m==7.1.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mregex\u001b[0m\u001b[2m==2024.11.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mregex\u001b[0m\u001b[2m==2025.9.18\u001b[0m\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m target-lexicon v0.12.16\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m once_cell v1.21.3\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m proc-macro2 v1.0.101\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m unicode-ident v1.0.18\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m libc v0.2.175\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m pyo3-build-config v0.23.5\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m crossbeam-utils v0.8.21\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m quote v1.0.40\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m pyo3-macros-backend v0.23.5\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m pyo3-ffi v0.23.5\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m autocfg v1.5.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m cfg-if v1.0.3\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m syn v2.0.106\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m memoffset v0.9.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m heck v0.5.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m rustversion v1.0.22\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m crossbeam-epoch v0.9.18\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m pyo3 v0.23.5\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m rayon-core v1.13.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m zerocopy v0.8.26\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m version_check v0.9.5\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m getrandom v0.3.3\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m memchr v2.7.5\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m aho-corasick v1.1.3\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m pyo3-macros v0.23.5\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m ahash v0.8.12\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m crossbeam-deque v0.8.6\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m indoc v2.0.6\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m bit-vec v0.8.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m regex-syntax v0.8.6\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m unindent v0.2.4\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m regex-automata v0.4.10\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m bit-set v0.8.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m castaway v0.2.4\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m log v0.4.28\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m arc-swap v1.7.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m either v1.15.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m static_assertions v1.1.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m ryu v1.0.20\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m hashbrown v0.15.5\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m itoa v1.0.15\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m equivalent v1.0.2\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m indexmap v2.11.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m compact_str v0.9.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m rayon v1.11.0\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m pyo3-log v0.12.4\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m fancy-regex v0.16.1\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m dary_heap v0.3.7\n",
            "\u001b[1m\u001b[32m   Compiling\u001b[0m rustbpe v0.1.0 (/content/nanochat/rustbpe)\n",
            "\u001b[1m\u001b[32m    Finished\u001b[0m `release` profile [optimized] target(s) in 59.95s\n",
            "üì¶ Built wheel for CPython 3.10 to /tmp/.tmpvlUD9L/nanochat-0.1.0-cp310-cp310-linux_x86_64.whl\n",
            "‚úèÔ∏è Setting installed package as editable\n",
            "üõ† Installed nanochat-0.1.0\n",
            "Downloading 8 shards using 4 workers...\n",
            "Target directory: /root/.cache/nanochat/base_data\n",
            "\n",
            "Downloading shard_00000.parquet...\n",
            "Downloading shard_00001.parquet...\n",
            "Downloading shard_00002.parquet...\n",
            "Downloading shard_00003.parquet...\n",
            "Successfully downloaded shard_00002.parquet\n",
            "Downloading shard_00004.parquet...\n",
            "Successfully downloaded shard_00003.parquet\n",
            "Downloading shard_00005.parquet...\n",
            "Successfully downloaded shard_00001.parquet\n",
            "Downloading shard_00006.parquet...\n",
            "Successfully downloaded shard_00000.parquet\n",
            "Downloading shard_00007.parquet...\n",
            "Successfully downloaded shard_00005.parquet\n",
            "Successfully downloaded shard_00004.parquet\n",
            "Successfully downloaded shard_00006.parquet\n",
            "Successfully downloaded shard_00007.parquet\n",
            "Done! Downloaded: 8/8 shards to /root/.cache/nanochat/base_data\n",
            "Downloading 240 shards using 4 workers...\n",
            "Target directory: /root/.cache/nanochat/base_data\n",
            "\n",
            "Skipping /root/.cache/nanochat/base_data/shard_00000.parquet (already exists)\n",
            "Skipping /root/.cache/nanochat/base_data/shard_00001.parquet (already exists)\n",
            "Skipping /root/.cache/nanochat/base_data/shard_00002.parquet (already exists)\n",
            "Skipping /root/.cache/nanochat/base_data/shard_00003.parquet (already exists)\n",
            "Skipping /root/.cache/nanochat/base_data/shard_00004.parquet (already exists)\n",
            "Skipping /root/.cache/nanochat/base_data/shard_00005.parquet (already exists)\n",
            "Skipping /root/.cache/nanochat/base_data/shard_00006.parquet (already exists)\n",
            "Skipping /root/.cache/nanochat/base_data/shard_00007.parquet (already exists)\n",
            "Downloading shard_00015.parquet...\n",
            "Downloading shard_00030.parquet...\n",
            "Downloading shard_00008.parquet...\n",
            "Downloading shard_00045.parquet...\n",
            "max_chars: 2,000,000,000\n",
            "doc_cap: 10,000\n",
            "vocab_size: 65,536\n",
            "2025-10-16 15:13:50,150 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Processing sequences from iterator (buffer_size: 8192)\n",
            "Successfully downloaded shard_00008.parquet\n",
            "Downloading shard_00009.parquet...\n",
            "Successfully downloaded shard_00015.parquet\n",
            "Downloading shard_00016.parquet...\n",
            "Successfully downloaded shard_00030.parquet\n",
            "Downloading shard_00031.parquet...\n",
            "Successfully downloaded shard_00045.parquet\n",
            "Downloading shard_00046.parquet...\n",
            "Successfully downloaded shard_00009.parquet\n",
            "Downloading shard_00010.parquet...\n",
            "Successfully downloaded shard_00016.parquet\n",
            "Successfully downloaded shard_00031.parquet\n",
            "Downloading shard_00017.parquet...\n",
            "Downloading shard_00032.parquet...\n",
            "Successfully downloaded shard_00046.parquet\n",
            "Downloading shard_00047.parquet...\n",
            "Successfully downloaded shard_00010.parquet\n",
            "Downloading shard_00011.parquet...\n",
            "Successfully downloaded shard_00032.parquet\n",
            "Downloading shard_00033.parquet...\n",
            "Successfully downloaded shard_00017.parquet\n",
            "Downloading shard_00018.parquet...\n",
            "Successfully downloaded shard_00047.parquet\n",
            "Downloading shard_00048.parquet...\n",
            "Successfully downloaded shard_00011.parquet\n",
            "Downloading shard_00012.parquet...\n",
            "Successfully downloaded shard_00033.parquet\n",
            "Downloading shard_00034.parquet...\n",
            "Successfully downloaded shard_00018.parquet\n",
            "Downloading shard_00019.parquet...\n",
            "Successfully downloaded shard_00048.parquet\n",
            "Downloading shard_00049.parquet...\n",
            "Successfully downloaded shard_00049.parquet\n",
            "Downloading shard_00050.parquet...\n",
            "Successfully downloaded shard_00019.parquet\n",
            "Downloading shard_00020.parquet...\n",
            "Successfully downloaded shard_00012.parquet\n",
            "Successfully downloaded shard_00034.parquet\n",
            "Downloading shard_00013.parquet...\n",
            "Downloading shard_00035.parquet...\n",
            "Successfully downloaded shard_00020.parquet\n",
            "Downloading shard_00021.parquet...\n",
            "Successfully downloaded shard_00050.parquet\n",
            "Downloading shard_00051.parquet...\n",
            "Successfully downloaded shard_00035.parquet\n",
            "Downloading shard_00036.parquet...\n",
            "Successfully downloaded shard_00013.parquet\n",
            "Downloading shard_00014.parquet...\n",
            "Successfully downloaded shard_00051.parquet\n",
            "Downloading shard_00052.parquet...\n",
            "Successfully downloaded shard_00021.parquet\n",
            "Downloading shard_00022.parquet...\n",
            "Successfully downloaded shard_00036.parquet\n",
            "Downloading shard_00037.parquet...\n",
            "Successfully downloaded shard_00014.parquet\n",
            "Downloading shard_00060.parquet...\n",
            "Successfully downloaded shard_00052.parquet\n",
            "Downloading shard_00053.parquet...\n",
            "Successfully downloaded shard_00022.parquet\n",
            "Downloading shard_00023.parquet...\n",
            "Successfully downloaded shard_00037.parquet\n",
            "Successfully downloaded shard_00060.parquet\n",
            "Downloading shard_00038.parquet...\n",
            "Downloading shard_00061.parquet...\n",
            "Successfully downloaded shard_00053.parquet\n",
            "Downloading shard_00054.parquet...\n",
            "Successfully downloaded shard_00023.parquet\n",
            "Downloading shard_00024.parquet...\n",
            "Successfully downloaded shard_00038.parquet\n",
            "Downloading shard_00039.parquet...\n",
            "Successfully downloaded shard_00061.parquet\n",
            "Downloading shard_00062.parquet...\n",
            "Successfully downloaded shard_00054.parquet\n",
            "Downloading shard_00055.parquet...\n",
            "Successfully downloaded shard_00024.parquet\n",
            "Downloading shard_00025.parquet...\n",
            "Successfully downloaded shard_00039.parquet\n",
            "Downloading shard_00040.parquet...\n",
            "Successfully downloaded shard_00055.parquet\n",
            "Downloading shard_00056.parquet...\n",
            "Successfully downloaded shard_00062.parquet\n",
            "Downloading shard_00063.parquet...\n",
            "Successfully downloaded shard_00025.parquet\n",
            "Downloading shard_00026.parquet...\n",
            "Successfully downloaded shard_00040.parquet\n",
            "Downloading shard_00041.parquet...\n",
            "Successfully downloaded shard_00056.parquet\n",
            "Downloading shard_00057.parquet...\n",
            "Successfully downloaded shard_00063.parquet\n",
            "Downloading shard_00064.parquet...\n",
            "Successfully downloaded shard_00026.parquet\n",
            "Downloading shard_00027.parquet...\n",
            "Successfully downloaded shard_00041.parquet\n",
            "Downloading shard_00042.parquet...\n",
            "Successfully downloaded shard_00057.parquet\n",
            "Downloading shard_00058.parquet...\n",
            "Successfully downloaded shard_00064.parquet\n",
            "Downloading shard_00065.parquet...\n",
            "Successfully downloaded shard_00027.parquet\n",
            "Downloading shard_00028.parquet...\n",
            "Successfully downloaded shard_00065.parquet\n",
            "Downloading shard_00066.parquet...\n",
            "Successfully downloaded shard_00042.parquet\n",
            "Downloading shard_00043.parquet...\n",
            "Successfully downloaded shard_00058.parquet\n",
            "Downloading shard_00059.parquet...\n",
            "Successfully downloaded shard_00028.parquet\n",
            "Downloading shard_00029.parquet...\n",
            "Successfully downloaded shard_00066.parquet\n",
            "Downloading shard_00067.parquet...\n",
            "Successfully downloaded shard_00043.parquet\n",
            "Downloading shard_00044.parquet...\n",
            "Successfully downloaded shard_00059.parquet\n",
            "Downloading shard_00075.parquet...\n",
            "Successfully downloaded shard_00029.parquet\n",
            "Downloading shard_00090.parquet...\n",
            "Successfully downloaded shard_00044.parquet\n",
            "Downloading shard_00105.parquet...\n",
            "Successfully downloaded shard_00067.parquet\n",
            "Downloading shard_00068.parquet...\n",
            "Successfully downloaded shard_00075.parquet\n",
            "Downloading shard_00076.parquet...\n",
            "Successfully downloaded shard_00090.parquet\n",
            "Downloading shard_00091.parquet...\n",
            "Successfully downloaded shard_00068.parquet\n",
            "Downloading shard_00069.parquet...\n",
            "Successfully downloaded shard_00105.parquet\n",
            "Downloading shard_00106.parquet...\n",
            "Successfully downloaded shard_00076.parquet\n",
            "Downloading shard_00077.parquet...\n",
            "Successfully downloaded shard_00106.parquet\n",
            "Downloading shard_00107.parquet...\n",
            "Successfully downloaded shard_00091.parquet\n",
            "Downloading shard_00092.parquet...\n",
            "Successfully downloaded shard_00069.parquet\n",
            "Downloading shard_00070.parquet...\n",
            "Successfully downloaded shard_00077.parquet\n",
            "Downloading shard_00078.parquet...\n",
            "Successfully downloaded shard_00070.parquet\n",
            "Downloading shard_00071.parquet...\n",
            "Successfully downloaded shard_00092.parquet\n",
            "Successfully downloaded shard_00107.parquet\n",
            "Downloading shard_00093.parquet...\n",
            "Downloading shard_00108.parquet...\n",
            "Successfully downloaded shard_00078.parquet\n",
            "Downloading shard_00079.parquet...\n",
            "Successfully downloaded shard_00071.parquet\n",
            "Downloading shard_00072.parquet...\n",
            "Successfully downloaded shard_00108.parquet\n",
            "Downloading shard_00109.parquet...\n",
            "Successfully downloaded shard_00093.parquet\n",
            "Downloading shard_00094.parquet...\n",
            "Successfully downloaded shard_00079.parquet\n",
            "Downloading shard_00080.parquet...\n",
            "Successfully downloaded shard_00109.parquet\n",
            "Downloading shard_00110.parquet...\n",
            "Successfully downloaded shard_00072.parquet\n",
            "Downloading shard_00073.parquet...\n",
            "Successfully downloaded shard_00094.parquet\n",
            "Downloading shard_00095.parquet...\n",
            "Successfully downloaded shard_00080.parquet\n",
            "Downloading shard_00081.parquet...\n",
            "Successfully downloaded shard_00110.parquet\n",
            "Downloading shard_00111.parquet...\n",
            "Successfully downloaded shard_00095.parquet\n",
            "Downloading shard_00096.parquet...\n",
            "Successfully downloaded shard_00073.parquet\n",
            "Downloading shard_00074.parquet...\n",
            "Successfully downloaded shard_00081.parquet\n",
            "Downloading shard_00082.parquet...\n",
            "Successfully downloaded shard_00111.parquet\n",
            "Downloading shard_00112.parquet...\n",
            "Successfully downloaded shard_00096.parquet\n",
            "Downloading shard_00097.parquet...\n",
            "Successfully downloaded shard_00082.parquet\n",
            "Downloading shard_00083.parquet...\n",
            "Successfully downloaded shard_00074.parquet\n",
            "Downloading shard_00120.parquet...\n",
            "Successfully downloaded shard_00112.parquet\n",
            "Downloading shard_00113.parquet...\n",
            "Successfully downloaded shard_00097.parquet\n",
            "Downloading shard_00098.parquet...\n",
            "Successfully downloaded shard_00120.parquet\n",
            "Downloading shard_00121.parquet...\n",
            "Successfully downloaded shard_00113.parquet\n",
            "Downloading shard_00114.parquet...\n",
            "Successfully downloaded shard_00083.parquet\n",
            "Downloading shard_00084.parquet...\n",
            "Successfully downloaded shard_00098.parquet\n",
            "Downloading shard_00099.parquet...\n",
            "Successfully downloaded shard_00114.parquet\n",
            "Downloading shard_00115.parquet...\n",
            "Successfully downloaded shard_00121.parquet\n",
            "Downloading shard_00122.parquet...\n",
            "Successfully downloaded shard_00084.parquet\n",
            "Downloading shard_00085.parquet...\n",
            "Successfully downloaded shard_00099.parquet\n",
            "Downloading shard_00100.parquet...\n",
            "Successfully downloaded shard_00122.parquet\n",
            "Downloading shard_00123.parquet...\n",
            "Successfully downloaded shard_00115.parquet\n",
            "Downloading shard_00116.parquet...\n",
            "Successfully downloaded shard_00085.parquet\n",
            "Downloading shard_00086.parquet...\n",
            "Successfully downloaded shard_00100.parquet\n",
            "Downloading shard_00101.parquet...\n",
            "Successfully downloaded shard_00123.parquet\n",
            "Downloading shard_00124.parquet...\n",
            "Successfully downloaded shard_00116.parquet\n",
            "Downloading shard_00117.parquet...\n",
            "Successfully downloaded shard_00086.parquet\n",
            "Downloading shard_00087.parquet...\n",
            "Successfully downloaded shard_00101.parquet\n",
            "Downloading shard_00102.parquet...\n",
            "Successfully downloaded shard_00124.parquet\n",
            "Downloading shard_00125.parquet...\n",
            "Successfully downloaded shard_00117.parquet\n",
            "Downloading shard_00118.parquet...\n",
            "Successfully downloaded shard_00087.parquet\n",
            "Downloading shard_00088.parquet...\n",
            "Successfully downloaded shard_00102.parquet\n",
            "Downloading shard_00103.parquet...\n",
            "Successfully downloaded shard_00125.parquet\n",
            "Downloading shard_00126.parquet...\n",
            "Successfully downloaded shard_00118.parquet\n",
            "Downloading shard_00119.parquet...\n",
            "Successfully downloaded shard_00088.parquet\n",
            "Downloading shard_00089.parquet...\n",
            "Successfully downloaded shard_00103.parquet\n",
            "Downloading shard_00104.parquet...\n",
            "Successfully downloaded shard_00126.parquet\n",
            "Downloading shard_00127.parquet...\n",
            "Successfully downloaded shard_00119.parquet\n",
            "Downloading shard_00135.parquet...\n",
            "Successfully downloaded shard_00104.parquet\n",
            "Successfully downloaded shard_00127.parquet\n",
            "Downloading shard_00150.parquet...\n",
            "Downloading shard_00128.parquet...\n",
            "Successfully downloaded shard_00089.parquet\n",
            "Downloading shard_00165.parquet...\n",
            "Successfully downloaded shard_00135.parquet\n",
            "Downloading shard_00136.parquet...\n",
            "Successfully downloaded shard_00150.parquet\n",
            "Downloading shard_00151.parquet...\n",
            "Successfully downloaded shard_00128.parquet\n",
            "Downloading shard_00129.parquet...\n",
            "Successfully downloaded shard_00165.parquet\n",
            "Downloading shard_00166.parquet...\n",
            "Successfully downloaded shard_00136.parquet\n",
            "Downloading shard_00137.parquet...\n",
            "Successfully downloaded shard_00151.parquet\n",
            "Downloading shard_00152.parquet...\n",
            "Successfully downloaded shard_00129.parquet\n",
            "Downloading shard_00130.parquet...\n",
            "Successfully downloaded shard_00166.parquet\n",
            "Downloading shard_00167.parquet...\n",
            "Successfully downloaded shard_00137.parquet\n",
            "Downloading shard_00138.parquet...\n",
            "Successfully downloaded shard_00152.parquet\n",
            "Downloading shard_00153.parquet...\n",
            "Successfully downloaded shard_00130.parquet\n",
            "Downloading shard_00131.parquet...\n",
            "Successfully downloaded shard_00167.parquet\n",
            "Downloading shard_00168.parquet...\n",
            "Successfully downloaded shard_00138.parquet\n",
            "Downloading shard_00139.parquet...\n",
            "Successfully downloaded shard_00153.parquet\n",
            "Downloading shard_00154.parquet...\n",
            "Successfully downloaded shard_00168.parquet\n",
            "Downloading shard_00169.parquet...\n",
            "Successfully downloaded shard_00131.parquet\n",
            "Downloading shard_00132.parquet...\n",
            "Successfully downloaded shard_00139.parquet\n",
            "Downloading shard_00140.parquet...\n",
            "Successfully downloaded shard_00154.parquet\n",
            "Downloading shard_00155.parquet...\n",
            "Successfully downloaded shard_00169.parquet\n",
            "Downloading shard_00170.parquet...\n",
            "Successfully downloaded shard_00132.parquet\n",
            "Downloading shard_00133.parquet...\n",
            "Successfully downloaded shard_00155.parquet\n",
            "Successfully downloaded shard_00140.parquet\n",
            "Downloading shard_00156.parquet...\n",
            "Downloading shard_00141.parquet...\n",
            "Successfully downloaded shard_00170.parquet\n",
            "Downloading shard_00171.parquet...\n",
            "Successfully downloaded shard_00133.parquet\n",
            "Downloading shard_00134.parquet...\n",
            "Successfully downloaded shard_00156.parquet\n",
            "Downloading shard_00157.parquet...\n",
            "Successfully downloaded shard_00141.parquet\n",
            "Downloading shard_00142.parquet...\n",
            "Successfully downloaded shard_00134.parquet\n",
            "Successfully downloaded shard_00171.parquet\n",
            "Downloading shard_00180.parquet...\n",
            "Downloading shard_00172.parquet...\n",
            "Successfully downloaded shard_00157.parquet\n",
            "Downloading shard_00158.parquet...\n",
            "Successfully downloaded shard_00142.parquet\n",
            "Successfully downloaded shard_00180.parquet\n",
            "Downloading shard_00143.parquet...\n",
            "Downloading shard_00181.parquet...\n",
            "Successfully downloaded shard_00172.parquet\n",
            "Downloading shard_00173.parquet...\n",
            "Successfully downloaded shard_00158.parquet\n",
            "Downloading shard_00159.parquet...\n",
            "Successfully downloaded shard_00143.parquet\n",
            "Downloading shard_00144.parquet...\n",
            "Successfully downloaded shard_00181.parquet\n",
            "Downloading shard_00182.parquet...\n",
            "Successfully downloaded shard_00173.parquet\n",
            "Downloading shard_00174.parquet...\n",
            "Successfully downloaded shard_00159.parquet\n",
            "Downloading shard_00160.parquet...\n",
            "Successfully downloaded shard_00144.parquet\n",
            "Successfully downloaded shard_00182.parquet\n",
            "Downloading shard_00145.parquet...\n",
            "Downloading shard_00183.parquet...\n",
            "Successfully downloaded shard_00174.parquet\n",
            "Downloading shard_00175.parquet...\n",
            "Successfully downloaded shard_00160.parquet\n",
            "Downloading shard_00161.parquet...\n",
            "Successfully downloaded shard_00145.parquet\n",
            "Downloading shard_00146.parquet...\n",
            "Successfully downloaded shard_00183.parquet\n",
            "Downloading shard_00184.parquet...\n",
            "Successfully downloaded shard_00175.parquet\n",
            "Downloading shard_00176.parquet...\n",
            "Successfully downloaded shard_00146.parquet\n",
            "Downloading shard_00147.parquet...\n",
            "Successfully downloaded shard_00161.parquet\n",
            "Downloading shard_00162.parquet...\n",
            "Successfully downloaded shard_00184.parquet\n",
            "Downloading shard_00185.parquet...\n",
            "Successfully downloaded shard_00176.parquet\n",
            "Downloading shard_00177.parquet...\n",
            "Successfully downloaded shard_00185.parquet\n",
            "Downloading shard_00186.parquet...\n",
            "Successfully downloaded shard_00147.parquet\n",
            "Downloading shard_00148.parquet...\n",
            "Successfully downloaded shard_00162.parquet\n",
            "Downloading shard_00163.parquet...\n",
            "Successfully downloaded shard_00177.parquet\n",
            "Downloading shard_00178.parquet...\n",
            "Successfully downloaded shard_00186.parquet\n",
            "Downloading shard_00187.parquet...\n",
            "Successfully downloaded shard_00178.parquet\n",
            "Downloading shard_00179.parquet...\n",
            "Successfully downloaded shard_00163.parquet\n",
            "Downloading shard_00164.parquet...\n",
            "Successfully downloaded shard_00148.parquet\n",
            "Downloading shard_00149.parquet...\n",
            "Successfully downloaded shard_00187.parquet\n",
            "Downloading shard_00188.parquet...\n",
            "Successfully downloaded shard_00179.parquet\n",
            "Downloading shard_00195.parquet...\n",
            "Successfully downloaded shard_00164.parquet\n",
            "Downloading shard_00210.parquet...\n",
            "Successfully downloaded shard_00149.parquet\n",
            "Downloading shard_00225.parquet...\n",
            "Successfully downloaded shard_00188.parquet\n",
            "Downloading shard_00189.parquet...\n",
            "Successfully downloaded shard_00210.parquet\n",
            "Successfully downloaded shard_00195.parquet\n",
            "Downloading shard_00211.parquet...\n",
            "Downloading shard_00196.parquet...\n",
            "Successfully downloaded shard_00225.parquet\n",
            "Downloading shard_00226.parquet...\n",
            "Successfully downloaded shard_00189.parquet\n",
            "Downloading shard_00190.parquet...\n",
            "Successfully downloaded shard_00211.parquet\n",
            "Downloading shard_00212.parquet...\n",
            "Successfully downloaded shard_00196.parquet\n",
            "Downloading shard_00197.parquet...\n",
            "Successfully downloaded shard_00226.parquet\n",
            "Downloading shard_00227.parquet...\n",
            "Successfully downloaded shard_00190.parquet\n",
            "Downloading shard_00191.parquet...\n",
            "Successfully downloaded shard_00212.parquet\n",
            "Downloading shard_00213.parquet...\n",
            "Successfully downloaded shard_00197.parquet\n",
            "Downloading shard_00198.parquet...\n",
            "Successfully downloaded shard_00191.parquet\n",
            "Successfully downloaded shard_00227.parquet\n",
            "Downloading shard_00228.parquet...\n",
            "Downloading shard_00192.parquet...\n",
            "Successfully downloaded shard_00213.parquet\n",
            "Downloading shard_00214.parquet...\n",
            "Successfully downloaded shard_00198.parquet\n",
            "Successfully downloaded shard_00228.parquet\n",
            "Downloading shard_00199.parquet...\n",
            "Downloading shard_00229.parquet...\n",
            "Successfully downloaded shard_00192.parquet\n",
            "Downloading shard_00193.parquet...\n",
            "Successfully downloaded shard_00214.parquet\n",
            "Downloading shard_00215.parquet...\n",
            "Successfully downloaded shard_00229.parquet\n",
            "Downloading shard_00230.parquet...\n",
            "Successfully downloaded shard_00193.parquet\n",
            "Downloading shard_00194.parquet...\n",
            "Successfully downloaded shard_00215.parquet\n",
            "Downloading shard_00216.parquet...\n",
            "Successfully downloaded shard_00199.parquet\n",
            "Downloading shard_00200.parquet...\n",
            "Successfully downloaded shard_00230.parquet\n",
            "Downloading shard_00231.parquet...\n",
            "Successfully downloaded shard_00216.parquet\n",
            "Downloading shard_00217.parquet...\n",
            "Successfully downloaded shard_00194.parquet\n",
            "Successfully downloaded shard_00200.parquet\n",
            "Downloading shard_00201.parquet...\n",
            "Successfully downloaded shard_00231.parquet\n",
            "Downloading shard_00232.parquet...\n",
            "Successfully downloaded shard_00217.parquet\n",
            "Downloading shard_00218.parquet...\n",
            "Successfully downloaded shard_00201.parquet\n",
            "Downloading shard_00202.parquet...\n",
            "Successfully downloaded shard_00232.parquet\n",
            "Downloading shard_00233.parquet...\n",
            "Successfully downloaded shard_00218.parquet\n",
            "Downloading shard_00219.parquet...\n",
            "Successfully downloaded shard_00202.parquet\n",
            "Downloading shard_00203.parquet...\n",
            "Successfully downloaded shard_00233.parquet\n",
            "Downloading shard_00234.parquet...\n",
            "Successfully downloaded shard_00219.parquet\n",
            "Downloading shard_00220.parquet...\n",
            "Successfully downloaded shard_00234.parquet\n",
            "Downloading shard_00235.parquet...\n",
            "Successfully downloaded shard_00203.parquet\n",
            "Downloading shard_00204.parquet...\n",
            "Successfully downloaded shard_00220.parquet\n",
            "Downloading shard_00221.parquet...\n",
            "Successfully downloaded shard_00235.parquet\n",
            "Downloading shard_00236.parquet...\n",
            "Successfully downloaded shard_00204.parquet\n",
            "Downloading shard_00205.parquet...\n",
            "Successfully downloaded shard_00221.parquet\n",
            "Downloading shard_00222.parquet...\n",
            "Successfully downloaded shard_00236.parquet\n",
            "Downloading shard_00237.parquet...\n",
            "Successfully downloaded shard_00205.parquet\n",
            "Downloading shard_00206.parquet...\n",
            "Successfully downloaded shard_00222.parquet\n",
            "Downloading shard_00223.parquet...\n",
            "Successfully downloaded shard_00237.parquet\n",
            "Downloading shard_00238.parquet...\n",
            "Successfully downloaded shard_00206.parquet\n",
            "Downloading shard_00207.parquet...\n",
            "Successfully downloaded shard_00223.parquet\n",
            "Downloading shard_00224.parquet...\n",
            "Successfully downloaded shard_00238.parquet\n",
            "Downloading shard_00239.parquet...\n",
            "Successfully downloaded shard_00207.parquet\n",
            "Downloading shard_00208.parquet...\n",
            "Successfully downloaded shard_00224.parquet\n",
            "Successfully downloaded shard_00239.parquet\n",
            "Successfully downloaded shard_00208.parquet\n",
            "Downloading shard_00209.parquet...\n",
            "Successfully downloaded shard_00209.parquet\n",
            "Done! Downloaded: 240/240 shards to /root/.cache/nanochat/base_data\n",
            "2025-10-16 15:20:42,905 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Processed 373760 sequences total, 1788540 unique\n",
            "2025-10-16 15:20:43,135 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Starting BPE training: 65271 merges to compute\n",
            "2025-10-16 15:20:43,135 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Computing initial pair counts from 1788540 unique sequences\n",
            "2025-10-16 15:20:47,630 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Building heap with 17546 unique pairs\n",
            "2025-10-16 15:20:47,633 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Starting merge loop\n",
            "2025-10-16 15:20:51,970 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m1%\u001b[0m (653/65271 merges) - Last merge: (32, 568) -> 908 (frequency: 179515)\n",
            "2025-10-16 15:20:52,568 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m2%\u001b[0m (1306/65271 merges) - Last merge: (259, 97) -> 1561 (frequency: 77269)\n",
            "2025-10-16 15:20:52,948 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m3%\u001b[0m (1959/65271 merges) - Last merge: (304, 339) -> 2214 (frequency: 47092)\n",
            "2025-10-16 15:20:53,298 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m4%\u001b[0m (2611/65271 merges) - Last merge: (111, 568) -> 2866 (frequency: 32580)\n",
            "2025-10-16 15:20:53,576 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m5%\u001b[0m (3264/65271 merges) - Last merge: (341, 469) -> 3519 (frequency: 24414)\n",
            "2025-10-16 15:20:53,794 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m6%\u001b[0m (3917/65271 merges) - Last merge: (2551, 297) -> 4172 (frequency: 19122)\n",
            "2025-10-16 15:20:53,978 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m7%\u001b[0m (4569/65271 merges) - Last merge: (436, 1244) -> 4824 (frequency: 15555)\n",
            "2025-10-16 15:20:54,187 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m8%\u001b[0m (5222/65271 merges) - Last merge: (3440, 111) -> 5477 (frequency: 12914)\n",
            "2025-10-16 15:20:54,425 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m9%\u001b[0m (5875/65271 merges) - Last merge: (278, 5650) -> 6130 (frequency: 10921)\n",
            "2025-10-16 15:20:54,658 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m10%\u001b[0m (6528/65271 merges) - Last merge: (84, 3509) -> 6783 (frequency: 9375)\n",
            "2025-10-16 15:20:54,808 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m11%\u001b[0m (7180/65271 merges) - Last merge: (71, 319) -> 7435 (frequency: 8139)\n",
            "2025-10-16 15:20:54,956 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m12%\u001b[0m (7833/65271 merges) - Last merge: (93, 46) -> 8088 (frequency: 7155)\n",
            "2025-10-16 15:20:55,071 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m13%\u001b[0m (8486/65271 merges) - Last merge: (264, 5416) -> 8741 (frequency: 6323)\n",
            "2025-10-16 15:20:55,218 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m14%\u001b[0m (9138/65271 merges) - Last merge: (4965, 1487) -> 9393 (frequency: 5627)\n",
            "2025-10-16 15:20:55,339 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m15%\u001b[0m (9791/65271 merges) - Last merge: (2649, 892) -> 10046 (frequency: 5067)\n",
            "2025-10-16 15:20:55,488 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m16%\u001b[0m (10444/65271 merges) - Last merge: (7036, 121) -> 10699 (frequency: 4581)\n",
            "2025-10-16 15:20:55,620 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m17%\u001b[0m (11097/65271 merges) - Last merge: (8202, 665) -> 11352 (frequency: 4135)\n",
            "2025-10-16 15:20:55,723 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m18%\u001b[0m (11749/65271 merges) - Last merge: (388, 322) -> 12004 (frequency: 3774)\n",
            "2025-10-16 15:20:55,826 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m19%\u001b[0m (12402/65271 merges) - Last merge: (422, 115) -> 12657 (frequency: 3486)\n",
            "2025-10-16 15:20:55,911 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m20%\u001b[0m (13055/65271 merges) - Last merge: (7436, 680) -> 13310 (frequency: 3224)\n",
            "2025-10-16 15:20:56,008 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m21%\u001b[0m (13707/65271 merges) - Last merge: (3557, 265) -> 13962 (frequency: 3000)\n",
            "2025-10-16 15:20:56,117 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m22%\u001b[0m (14360/65271 merges) - Last merge: (1223, 2907) -> 14615 (frequency: 2782)\n",
            "2025-10-16 15:20:56,180 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m23%\u001b[0m (15013/65271 merges) - Last merge: (83, 67) -> 15268 (frequency: 2596)\n",
            "2025-10-16 15:20:56,253 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m24%\u001b[0m (15666/65271 merges) - Last merge: (1037, 1153) -> 15921 (frequency: 2421)\n",
            "2025-10-16 15:20:56,340 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m25%\u001b[0m (16318/65271 merges) - Last merge: (340, 681) -> 16573 (frequency: 2268)\n",
            "2025-10-16 15:20:56,455 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m26%\u001b[0m (16971/65271 merges) - Last merge: (295, 1980) -> 17226 (frequency: 2125)\n",
            "2025-10-16 15:20:56,524 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m27%\u001b[0m (17624/65271 merges) - Last merge: (16547, 4763) -> 17879 (frequency: 2004)\n",
            "2025-10-16 15:20:56,589 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m28%\u001b[0m (18276/65271 merges) - Last merge: (264, 15567) -> 18531 (frequency: 1891)\n",
            "2025-10-16 15:20:56,640 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m29%\u001b[0m (18929/65271 merges) - Last merge: (14831, 2223) -> 19184 (frequency: 1788)\n",
            "2025-10-16 15:20:56,705 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m30%\u001b[0m (19582/65271 merges) - Last merge: (1617, 121) -> 19837 (frequency: 1687)\n",
            "2025-10-16 15:20:56,742 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m31%\u001b[0m (20235/65271 merges) - Last merge: (1529, 342) -> 20490 (frequency: 1599)\n",
            "2025-10-16 15:20:56,795 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m32%\u001b[0m (20887/65271 merges) - Last merge: (98, 5762) -> 21142 (frequency: 1513)\n",
            "2025-10-16 15:20:56,839 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m33%\u001b[0m (21540/65271 merges) - Last merge: (1750, 1632) -> 21795 (frequency: 1436)\n",
            "2025-10-16 15:20:56,883 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m34%\u001b[0m (22193/65271 merges) - Last merge: (647, 269) -> 22448 (frequency: 1364)\n",
            "2025-10-16 15:20:56,939 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m35%\u001b[0m (22845/65271 merges) - Last merge: (275, 587) -> 23100 (frequency: 1297)\n",
            "2025-10-16 15:20:56,977 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m36%\u001b[0m (23498/65271 merges) - Last merge: (1768, 10289) -> 23753 (frequency: 1233)\n",
            "2025-10-16 15:20:57,026 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m37%\u001b[0m (24151/65271 merges) - Last merge: (103, 551) -> 24406 (frequency: 1175)\n",
            "2025-10-16 15:20:57,073 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m38%\u001b[0m (24803/65271 merges) - Last merge: (18428, 380) -> 25058 (frequency: 1121)\n",
            "2025-10-16 15:20:57,121 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m39%\u001b[0m (25456/65271 merges) - Last merge: (1086, 9975) -> 25711 (frequency: 1072)\n",
            "2025-10-16 15:20:57,161 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m40%\u001b[0m (26109/65271 merges) - Last merge: (13645, 367) -> 26364 (frequency: 1027)\n",
            "2025-10-16 15:20:57,219 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m41%\u001b[0m (26762/65271 merges) - Last merge: (6190, 9886) -> 27017 (frequency: 985)\n",
            "2025-10-16 15:20:57,259 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m42%\u001b[0m (27414/65271 merges) - Last merge: (1602, 6054) -> 27669 (frequency: 943)\n",
            "2025-10-16 15:20:57,308 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m43%\u001b[0m (28067/65271 merges) - Last merge: (823, 359) -> 28322 (frequency: 903)\n",
            "2025-10-16 15:20:57,365 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m44%\u001b[0m (28720/65271 merges) - Last merge: (2515, 1194) -> 28975 (frequency: 867)\n",
            "2025-10-16 15:20:57,399 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m45%\u001b[0m (29372/65271 merges) - Last merge: (66, 371) -> 29627 (frequency: 834)\n",
            "2025-10-16 15:20:57,432 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m46%\u001b[0m (30025/65271 merges) - Last merge: (325, 925) -> 30280 (frequency: 802)\n",
            "2025-10-16 15:20:57,473 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m47%\u001b[0m (30678/65271 merges) - Last merge: (1079, 339) -> 30933 (frequency: 773)\n",
            "2025-10-16 15:20:57,505 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m48%\u001b[0m (31331/65271 merges) - Last merge: (2918, 15894) -> 31586 (frequency: 747)\n",
            "2025-10-16 15:20:57,538 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m49%\u001b[0m (31983/65271 merges) - Last merge: (4292, 10683) -> 32238 (frequency: 721)\n",
            "2025-10-16 15:20:57,572 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m50%\u001b[0m (32636/65271 merges) - Last merge: (69, 16225) -> 32891 (frequency: 695)\n",
            "2025-10-16 15:20:57,610 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m51%\u001b[0m (33289/65271 merges) - Last merge: (337, 271) -> 33544 (frequency: 671)\n",
            "2025-10-16 15:20:57,643 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m52%\u001b[0m (33941/65271 merges) - Last merge: (11796, 509) -> 34196 (frequency: 649)\n",
            "2025-10-16 15:20:57,669 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m53%\u001b[0m (34594/65271 merges) - Last merge: (8652, 326) -> 34849 (frequency: 627)\n",
            "2025-10-16 15:20:57,697 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m54%\u001b[0m (35247/65271 merges) - Last merge: (340, 259) -> 35502 (frequency: 606)\n",
            "2025-10-16 15:20:57,729 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m55%\u001b[0m (35900/65271 merges) - Last merge: (25892, 115) -> 36155 (frequency: 588)\n",
            "2025-10-16 15:20:57,757 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m56%\u001b[0m (36552/65271 merges) - Last merge: (283, 3480) -> 36807 (frequency: 569)\n",
            "2025-10-16 15:20:57,807 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m57%\u001b[0m (37205/65271 merges) - Last merge: (18415, 145) -> 37460 (frequency: 552)\n",
            "2025-10-16 15:20:57,851 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m58%\u001b[0m (37858/65271 merges) - Last merge: (818, 274) -> 38113 (frequency: 535)\n",
            "2025-10-16 15:20:57,881 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m59%\u001b[0m (38510/65271 merges) - Last merge: (317, 16928) -> 38765 (frequency: 519)\n",
            "2025-10-16 15:20:57,907 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m60%\u001b[0m (39163/65271 merges) - Last merge: (356, 18021) -> 39418 (frequency: 503)\n",
            "2025-10-16 15:20:57,931 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m61%\u001b[0m (39816/65271 merges) - Last merge: (10715, 469) -> 40071 (frequency: 489)\n",
            "2025-10-16 15:20:57,957 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m62%\u001b[0m (40469/65271 merges) - Last merge: (6256, 2262) -> 40724 (frequency: 475)\n",
            "2025-10-16 15:20:57,978 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m63%\u001b[0m (41121/65271 merges) - Last merge: (2385, 4266) -> 41376 (frequency: 460)\n",
            "2025-10-16 15:20:58,003 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m64%\u001b[0m (41774/65271 merges) - Last merge: (3162, 34983) -> 42029 (frequency: 447)\n",
            "2025-10-16 15:20:58,046 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m65%\u001b[0m (42427/65271 merges) - Last merge: (1731, 285) -> 42682 (frequency: 435)\n",
            "2025-10-16 15:20:58,068 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m66%\u001b[0m (43079/65271 merges) - Last merge: (1546, 35673) -> 43334 (frequency: 423)\n",
            "2025-10-16 15:20:58,105 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m67%\u001b[0m (43732/65271 merges) - Last merge: (314, 1090) -> 43987 (frequency: 411)\n",
            "2025-10-16 15:20:58,133 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m68%\u001b[0m (44385/65271 merges) - Last merge: (1016, 99) -> 44640 (frequency: 400)\n",
            "2025-10-16 15:20:58,150 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m69%\u001b[0m (45037/65271 merges) - Last merge: (15144, 302) -> 45292 (frequency: 390)\n",
            "2025-10-16 15:20:58,171 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m70%\u001b[0m (45690/65271 merges) - Last merge: (707, 32845) -> 45945 (frequency: 380)\n",
            "2025-10-16 15:20:58,199 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m71%\u001b[0m (46343/65271 merges) - Last merge: (342, 315) -> 46598 (frequency: 370)\n",
            "2025-10-16 15:20:58,222 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m72%\u001b[0m (46996/65271 merges) - Last merge: (31558, 130) -> 47251 (frequency: 362)\n",
            "2025-10-16 15:20:58,248 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m73%\u001b[0m (47648/65271 merges) - Last merge: (388, 4478) -> 47903 (frequency: 352)\n",
            "2025-10-16 15:20:58,270 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m74%\u001b[0m (48301/65271 merges) - Last merge: (20505, 289) -> 48556 (frequency: 344)\n",
            "2025-10-16 15:20:58,289 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m75%\u001b[0m (48954/65271 merges) - Last merge: (590, 460) -> 49209 (frequency: 335)\n",
            "2025-10-16 15:20:58,310 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m76%\u001b[0m (49606/65271 merges) - Last merge: (407, 31100) -> 49861 (frequency: 327)\n",
            "2025-10-16 15:20:58,330 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m77%\u001b[0m (50259/65271 merges) - Last merge: (13802, 89) -> 50514 (frequency: 320)\n",
            "2025-10-16 15:20:58,358 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m78%\u001b[0m (50912/65271 merges) - Last merge: (420, 12214) -> 51167 (frequency: 312)\n",
            "2025-10-16 15:20:58,377 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m79%\u001b[0m (51565/65271 merges) - Last merge: (8299, 13053) -> 51820 (frequency: 306)\n",
            "2025-10-16 15:20:58,393 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m80%\u001b[0m (52217/65271 merges) - Last merge: (3551, 3651) -> 52472 (frequency: 299)\n",
            "2025-10-16 15:20:58,415 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m81%\u001b[0m (52870/65271 merges) - Last merge: (49193, 528) -> 53125 (frequency: 293)\n",
            "2025-10-16 15:20:58,432 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m82%\u001b[0m (53523/65271 merges) - Last merge: (1074, 7277) -> 53778 (frequency: 286)\n",
            "2025-10-16 15:20:58,451 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m83%\u001b[0m (54175/65271 merges) - Last merge: (71, 1610) -> 54430 (frequency: 280)\n",
            "2025-10-16 15:20:58,470 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m84%\u001b[0m (54828/65271 merges) - Last merge: (277, 757) -> 55083 (frequency: 274)\n",
            "2025-10-16 15:20:58,495 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m85%\u001b[0m (55481/65271 merges) - Last merge: (20664, 55720) -> 55736 (frequency: 269)\n",
            "2025-10-16 15:20:58,510 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m86%\u001b[0m (56134/65271 merges) - Last merge: (33914, 4479) -> 56389 (frequency: 263)\n",
            "2025-10-16 15:20:58,532 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m87%\u001b[0m (56786/65271 merges) - Last merge: (971, 22532) -> 57041 (frequency: 257)\n",
            "2025-10-16 15:20:58,548 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m88%\u001b[0m (57439/65271 merges) - Last merge: (13993, 4663) -> 57694 (frequency: 252)\n",
            "2025-10-16 15:20:58,566 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m89%\u001b[0m (58092/65271 merges) - Last merge: (4588, 47531) -> 58347 (frequency: 247)\n",
            "2025-10-16 15:20:58,585 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m90%\u001b[0m (58744/65271 merges) - Last merge: (10832, 46926) -> 58999 (frequency: 242)\n",
            "2025-10-16 15:20:58,606 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m91%\u001b[0m (59397/65271 merges) - Last merge: (20613, 596) -> 59652 (frequency: 237)\n",
            "2025-10-16 15:20:58,630 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m92%\u001b[0m (60050/65271 merges) - Last merge: (5209, 2426) -> 60305 (frequency: 232)\n",
            "2025-10-16 15:20:58,647 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m93%\u001b[0m (60703/65271 merges) - Last merge: (114, 518) -> 60958 (frequency: 227)\n",
            "2025-10-16 15:20:58,706 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m94%\u001b[0m (61355/65271 merges) - Last merge: (27173, 37056) -> 61610 (frequency: 223)\n",
            "2025-10-16 15:20:58,723 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m95%\u001b[0m (62008/65271 merges) - Last merge: (560, 9077) -> 62263 (frequency: 218)\n",
            "2025-10-16 15:20:58,739 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m96%\u001b[0m (62661/65271 merges) - Last merge: (9055, 736) -> 62916 (frequency: 214)\n",
            "2025-10-16 15:20:58,754 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m97%\u001b[0m (63313/65271 merges) - Last merge: (41763, 274) -> 63568 (frequency: 210)\n",
            "2025-10-16 15:20:58,772 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m98%\u001b[0m (63966/65271 merges) - Last merge: (10426, 10009) -> 64221 (frequency: 206)\n",
            "2025-10-16 15:20:58,793 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m99%\u001b[0m (64619/65271 merges) - Last merge: (407, 75) -> 64874 (frequency: 202)\n",
            "2025-10-16 15:20:58,806 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Progress: \u001b[1m100%\u001b[0m (65271/65271 merges) - Last merge: (26074, 107) -> 65526 (frequency: 199)\n",
            "2025-10-16 15:20:58,807 - rustbpe - \u001b[32m\u001b[1mINFO\u001b[0m - Finished training: 65271 merges completed\n",
            "Training time: 430.03s\n",
            "Saved tokenizer encoding to /root/.cache/nanochat/tokenizer/tokenizer.pkl\n",
            "Saved token_bytes to /root/.cache/nanochat/tokenizer/token_bytes.pt\n",
            "\n",
            "Vocab sizes:\n",
            "GPT-2: 50257\n",
            "GPT-4: 100277\n",
            "Ours: 65536\n",
            "\n",
            "Comparison with GPT-2:\n",
            "===============================================================================================\n",
            "Text Type  Bytes    GPT-2           Ours            Relative     Better    \n",
            "                    Tokens  Ratio   Tokens  Ratio   Diff %      \n",
            "-----------------------------------------------------------------------------------------------\n",
            "news       1819     \u001b[91m404    \u001b[0m \u001b[91m4.50   \u001b[0m \u001b[92m375    \u001b[0m \u001b[92m4.85   \u001b[0m \u001b[92m   +7.2%\u001b[0m     Ours      \n",
            "korean     893      \u001b[91m745    \u001b[0m \u001b[91m1.20   \u001b[0m \u001b[92m712    \u001b[0m \u001b[92m1.25   \u001b[0m \u001b[92m   +4.4%\u001b[0m     Ours      \n",
            "code       1259     \u001b[91m576    \u001b[0m \u001b[91m2.19   \u001b[0m \u001b[92m492    \u001b[0m \u001b[92m2.56   \u001b[0m \u001b[92m  +14.6%\u001b[0m     Ours      \n",
            "math       1834     \u001b[92m936    \u001b[0m \u001b[92m1.96   \u001b[0m \u001b[91m966    \u001b[0m \u001b[91m1.90   \u001b[0m \u001b[91m   -3.2%\u001b[0m     GPT-2     \n",
            "science    1112     \u001b[91m260    \u001b[0m \u001b[91m4.28   \u001b[0m \u001b[92m228    \u001b[0m \u001b[92m4.88   \u001b[0m \u001b[92m  +12.3%\u001b[0m     Ours      \n",
            "fwe-train  4208518  \u001b[91m900364 \u001b[0m \u001b[91m4.67   \u001b[0m \u001b[92m856883 \u001b[0m \u001b[92m4.91   \u001b[0m \u001b[92m   +4.8%\u001b[0m     Ours      \n",
            "fwe-val    4908443  \u001b[91m1059062\u001b[0m \u001b[91m4.63   \u001b[0m \u001b[92m1010352\u001b[0m \u001b[92m4.86   \u001b[0m \u001b[92m   +4.6%\u001b[0m     Ours      \n",
            "\n",
            "Comparison with GPT-4:\n",
            "===============================================================================================\n",
            "Text Type  Bytes    GPT-4           Ours            Relative     Better    \n",
            "                    Tokens  Ratio   Tokens  Ratio   Diff %      \n",
            "-----------------------------------------------------------------------------------------------\n",
            "news       1819     \u001b[91m387    \u001b[0m \u001b[91m4.70   \u001b[0m \u001b[92m375    \u001b[0m \u001b[92m4.85   \u001b[0m \u001b[92m   +3.1%\u001b[0m     Ours      \n",
            "korean     893      \u001b[92m364    \u001b[0m \u001b[92m2.45   \u001b[0m \u001b[91m712    \u001b[0m \u001b[91m1.25   \u001b[0m \u001b[91m  -95.6%\u001b[0m     GPT-4     \n",
            "code       1259     \u001b[92m309    \u001b[0m \u001b[92m4.07   \u001b[0m \u001b[91m492    \u001b[0m \u001b[91m2.56   \u001b[0m \u001b[91m  -59.2%\u001b[0m     GPT-4     \n",
            "math       1834     \u001b[92m832    \u001b[0m \u001b[92m2.20   \u001b[0m \u001b[91m966    \u001b[0m \u001b[91m1.90   \u001b[0m \u001b[91m  -16.1%\u001b[0m     GPT-4     \n",
            "science    1112     \u001b[91m249    \u001b[0m \u001b[91m4.47   \u001b[0m \u001b[92m228    \u001b[0m \u001b[92m4.88   \u001b[0m \u001b[92m   +8.4%\u001b[0m     Ours      \n",
            "fwe-train  4208518  \u001b[91m874799 \u001b[0m \u001b[91m4.81   \u001b[0m \u001b[92m856883 \u001b[0m \u001b[92m4.91   \u001b[0m \u001b[92m   +2.0%\u001b[0m     Ours      \n",
            "fwe-val    4908443  \u001b[91m1029691\u001b[0m \u001b[91m4.77   \u001b[0m \u001b[92m1010352\u001b[0m \u001b[92m4.86   \u001b[0m \u001b[92m   +1.9%\u001b[0m     Ours      \n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 24.8M  100 24.8M    0     0  27.8M      0 --:--:-- --:--:-- --:--:-- 27.8M\n",
            "Waiting for dataset download to complete...\n",
            "\n",
            "                                                   ‚ñà‚ñà‚ñà‚ñà‚ñà                 ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "                                                  ‚ñë‚ñë‚ñà‚ñà‚ñà                 ‚ñë‚ñë‚ñà‚ñà‚ñà\n",
            " ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  ‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "‚ñë‚ñë‚ñà‚ñà‚ñà‚ñë‚ñë‚ñà‚ñà‚ñà  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñë‚ñà‚ñà‚ñà‚ñë‚ñë‚ñà‚ñà‚ñà  ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñà‚ñà‚ñà‚ñë‚ñë‚ñà‚ñà‚ñà  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñë\n",
            " ‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  ‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñà‚ñà‚ñà‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñë‚ñë  ‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   ‚ñë‚ñà‚ñà‚ñà\n",
            " ‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñà‚ñà‚ñà  ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñà‚ñà‚ñà  ‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñà‚ñà‚ñà‚ñë‚ñà‚ñà‚ñà  ‚ñà‚ñà‚ñà ‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñà‚ñà‚ñà  ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñà‚ñà‚ñà   ‚ñë‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà\n",
            " ‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  ‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  ‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "‚ñë‚ñë‚ñë‚ñë ‚ñë‚ñë‚ñë‚ñë‚ñë  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë ‚ñë‚ñë‚ñë‚ñë ‚ñë‚ñë‚ñë‚ñë‚ñë  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  ‚ñë‚ñë‚ñë‚ñë ‚ñë‚ñë‚ñë‚ñë‚ñë  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë    ‚ñë‚ñë‚ñë‚ñë‚ñë\n",
            "\n",
            "Overriding: depth = 20\n",
            "Overriding: run = dummy\n",
            "2025-10-16 15:21:31,224 - nanochat.common - \u001b[32m\u001b[1mINFO\u001b[0m - Distributed world size: 1\n",
            "Vocab size: 65,536\n",
            "num_layers: 20\n",
            "model_dim: 1280\n",
            "num_heads: 10\n",
            "num_kv_heads: 10\n",
            "Tokens / micro-batch / rank: 32 x 2048 = 65,536\n",
            "Tokens / micro-batch: 65,536\n",
            "Total batch size 524,288 => gradient accumulation steps: 8\n",
            "Number of parameters: 560,988,160\n",
            "Estimated FLOPs per token: 3.491758e+09\n",
            "Calculated number of iterations from target data:param ratio: 21,400\n",
            "Total number of training tokens: 11,219,763,200\n",
            "Tokens : Params ratio: 20.00\n",
            "Total training FLOPs estimate: 3.917670e+19\n",
            "Scaling the LR for the AdamW parameters ‚àù1/‚àö(1280/768) = 0.774597\n",
            "Muon: Grouping 80 params of shape torch.Size([1280, 1280]), device cuda:0, dtype torch.float32\n",
            "Muon: Grouping 20 params of shape torch.Size([1280, 5120]), device cuda:0, dtype torch.float32\n",
            "Muon: Grouping 20 params of shape torch.Size([5120, 1280]), device cuda:0, dtype torch.float32\n",
            "[rank0]:W1016 15:21:46.240000 7114 .venv/lib/python3.10/site-packages/torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "/content/nanochat/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/content/nanochat/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/content/nanochat/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/content/nanochat/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/content/nanochat/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "[rank0]: Traceback (most recent call last):\n",
            "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "[rank0]:     return _run_code(code, main_globals, None,\n",
            "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "[rank0]:     exec(code, run_globals)\n",
            "[rank0]:   File \"/content/nanochat/scripts/base_train.py\", line 182, in <module>\n",
            "[rank0]:     val_bpb = evaluate_bpb(model, val_loader, eval_steps, token_bytes)\n",
            "[rank0]:   File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "[rank0]:     return func(*args, **kwargs)\n",
            "[rank0]:   File \"/content/nanochat/nanochat/loss_eval.py\", line 33, in evaluate_bpb\n",
            "[rank0]:     loss2d = model(x, y, loss_reduction='none') # (B, T)\n",
            "[rank0]:   File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 375, in __call__\n",
            "[rank0]:     return super().__call__(*args, **kwargs)\n",
            "[rank0]:   File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "[rank0]:     return self._call_impl(*args, **kwargs)\n",
            "[rank0]:   File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "[rank0]:     return forward_call(*args, **kwargs)\n",
            "[rank0]:   File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 736, in compile_wrapper\n",
            "[rank0]:     return fn(*args, **kwargs)\n",
            "[rank0]:   File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "[rank0]:     return self._call_impl(*args, **kwargs)\n",
            "[rank0]:   File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "[rank0]:     return forward_call(*args, **kwargs)\n",
            "[rank0]:   File \"/content/nanochat/nanochat/gpt.py\", line 274, in forward\n",
            "[rank0]:     x = block(x, cos_sin, kv_cache)\n",
            "[rank0]:   File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "[rank0]:     return self._call_impl(*args, **kwargs)\n",
            "[rank0]:   File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "[rank0]:     return forward_call(*args, **kwargs)\n",
            "[rank0]:   File \"/content/nanochat/nanochat/gpt.py\", line 149, in forward\n",
            "[rank0]:     x = x + self.attn(norm(x), cos_sin, kv_cache)\n",
            "[rank0]:   File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "[rank0]:     return self._call_impl(*args, **kwargs)\n",
            "[rank0]:   File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "[rank0]:     return forward_call(*args, **kwargs)\n",
            "[rank0]:   File \"/content/nanochat/nanochat/gpt.py\", line 107, in forward\n",
            "[rank0]:     y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
            "[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 0 has a total capacity of 14.74 GiB of which 68.12 MiB is free. Process 63904 has 14.67 GiB memory in use. Of the allocated memory 14.06 GiB is allocated by PyTorch, and 45.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "[rank0]:[W1016 15:21:53.661371922 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "E1016 15:21:55.392000 7102 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 7114) of binary: /content/nanochat/.venv/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanochat/.venv/bin/torchrun\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 901, in main\n",
            "    run(args)\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 892, in run\n",
            "    elastic_launch(\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 143, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 277, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "scripts.base_train FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2025-10-16_15:21:55\n",
            "  host      : 26106bffa2bf\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : 1 (pid: 7114)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n",
            "2025-10-16 15:22:02,810 - nanochat.common - \u001b[32m\u001b[1mINFO\u001b[0m - Distributed world size: 1\n",
            "[rank0]: Traceback (most recent call last):\n",
            "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "[rank0]:     return _run_code(code, main_globals, None,\n",
            "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "[rank0]:     exec(code, run_globals)\n",
            "[rank0]:   File \"/content/nanochat/scripts/base_loss.py\", line 27, in <module>\n",
            "[rank0]:     model, tokenizer, meta = load_model(\"base\", device, phase=\"eval\", model_tag=model_tag, step=model_step)\n",
            "[rank0]:   File \"/content/nanochat/nanochat/checkpoint_manager.py\", line 146, in load_model\n",
            "[rank0]:     return load_model_from_dir(checkpoints_dir, *args, **kwargs)\n",
            "[rank0]:   File \"/content/nanochat/nanochat/checkpoint_manager.py\", line 125, in load_model_from_dir\n",
            "[rank0]:     model_tag = find_largest_model(checkpoints_dir)\n",
            "[rank0]:   File \"/content/nanochat/nanochat/checkpoint_manager.py\", line 93, in find_largest_model\n",
            "[rank0]:     model_tags = [f for f in os.listdir(checkpoint_dir) if os.path.isdir(os.path.join(checkpoint_dir, f))]\n",
            "[rank0]: FileNotFoundError: [Errno 2] No such file or directory: '/root/.cache/nanochat/base_checkpoints'\n",
            "[rank0]:[W1016 15:22:05.838792142 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "E1016 15:22:07.082000 7320 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 7328) of binary: /content/nanochat/.venv/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanochat/.venv/bin/torchrun\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 901, in main\n",
            "    run(args)\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 892, in run\n",
            "    elastic_launch(\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 143, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 277, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "scripts.base_loss FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2025-10-16_15:22:07\n",
            "  host      : 26106bffa2bf\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : 1 (pid: 7328)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n",
            "2025-10-16 15:22:14,672 - nanochat.common - \u001b[32m\u001b[1mINFO\u001b[0m - Distributed world size: 1\n",
            "[rank0]: Traceback (most recent call last):\n",
            "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "[rank0]:     return _run_code(code, main_globals, None,\n",
            "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "[rank0]:     exec(code, run_globals)\n",
            "[rank0]:   File \"/content/nanochat/scripts/base_eval.py\", line 180, in <module>\n",
            "[rank0]:     main()\n",
            "[rank0]:   File \"/content/nanochat/scripts/base_eval.py\", line 137, in main\n",
            "[rank0]:     model, tokenizer, meta = load_model(\"base\", device, phase=\"eval\")\n",
            "[rank0]:   File \"/content/nanochat/nanochat/checkpoint_manager.py\", line 146, in load_model\n",
            "[rank0]:     return load_model_from_dir(checkpoints_dir, *args, **kwargs)\n",
            "[rank0]:   File \"/content/nanochat/nanochat/checkpoint_manager.py\", line 125, in load_model_from_dir\n",
            "[rank0]:     model_tag = find_largest_model(checkpoints_dir)\n",
            "[rank0]:   File \"/content/nanochat/nanochat/checkpoint_manager.py\", line 93, in find_largest_model\n",
            "[rank0]:     model_tags = [f for f in os.listdir(checkpoint_dir) if os.path.isdir(os.path.join(checkpoint_dir, f))]\n",
            "[rank0]: FileNotFoundError: [Errno 2] No such file or directory: '/root/.cache/nanochat/base_checkpoints'\n",
            "[rank0]:[W1016 15:22:17.804651807 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "E1016 15:22:18.898000 7393 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 7405) of binary: /content/nanochat/.venv/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanochat/.venv/bin/torchrun\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 901, in main\n",
            "    run(args)\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 892, in run\n",
            "    elastic_launch(\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 143, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 277, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "scripts.base_eval FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2025-10-16_15:22:18\n",
            "  host      : 26106bffa2bf\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : 1 (pid: 7405)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n",
            "Overriding: run = dummy\n",
            "2025-10-16 15:22:29,420 - nanochat.common - \u001b[32m\u001b[1mINFO\u001b[0m - Distributed world size: 1\n",
            "[rank0]: Traceback (most recent call last):\n",
            "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "[rank0]:     return _run_code(code, main_globals, None,\n",
            "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "[rank0]:     exec(code, run_globals)\n",
            "[rank0]:   File \"/content/nanochat/scripts/mid_train.py\", line 63, in <module>\n",
            "[rank0]:     model, tokenizer, meta = load_model(\"base\", device, phase=\"train\", model_tag=model_tag, step=step)\n",
            "[rank0]:   File \"/content/nanochat/nanochat/checkpoint_manager.py\", line 146, in load_model\n",
            "[rank0]:     return load_model_from_dir(checkpoints_dir, *args, **kwargs)\n",
            "[rank0]:   File \"/content/nanochat/nanochat/checkpoint_manager.py\", line 125, in load_model_from_dir\n",
            "[rank0]:     model_tag = find_largest_model(checkpoints_dir)\n",
            "[rank0]:   File \"/content/nanochat/nanochat/checkpoint_manager.py\", line 93, in find_largest_model\n",
            "[rank0]:     model_tags = [f for f in os.listdir(checkpoint_dir) if os.path.isdir(os.path.join(checkpoint_dir, f))]\n",
            "[rank0]: FileNotFoundError: [Errno 2] No such file or directory: '/root/.cache/nanochat/base_checkpoints'\n",
            "[rank0]:[W1016 15:22:31.121253824 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "E1016 15:22:33.184000 7468 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 7480) of binary: /content/nanochat/.venv/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanochat/.venv/bin/torchrun\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 901, in main\n",
            "    run(args)\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 892, in run\n",
            "    elastic_launch(\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 143, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 277, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "scripts.mid_train FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2025-10-16_15:22:33\n",
            "  host      : 26106bffa2bf\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : 1 (pid: 7480)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n",
            "2025-10-16 15:22:41,215 - nanochat.common - \u001b[32m\u001b[1mINFO\u001b[0m - Distributed world size: 1\n",
            "[rank0]: Traceback (most recent call last):\n",
            "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "[rank0]:     return _run_code(code, main_globals, None,\n",
            "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "[rank0]:     exec(code, run_globals)\n",
            "[rank0]:   File \"/content/nanochat/scripts/chat_eval.py\", line 200, in <module>\n",
            "[rank0]:     model, tokenizer, meta = load_model(args.source, device, phase=\"eval\", model_tag=args.model_tag, step=args.step)\n",
            "[rank0]:   File \"/content/nanochat/nanochat/checkpoint_manager.py\", line 146, in load_model\n",
            "[rank0]:     return load_model_from_dir(checkpoints_dir, *args, **kwargs)\n",
            "[rank0]:   File \"/content/nanochat/nanochat/checkpoint_manager.py\", line 125, in load_model_from_dir\n",
            "[rank0]:     model_tag = find_largest_model(checkpoints_dir)\n",
            "[rank0]:   File \"/content/nanochat/nanochat/checkpoint_manager.py\", line 93, in find_largest_model\n",
            "[rank0]:     model_tags = [f for f in os.listdir(checkpoint_dir) if os.path.isdir(os.path.join(checkpoint_dir, f))]\n",
            "[rank0]: FileNotFoundError: [Errno 2] No such file or directory: '/root/.cache/nanochat/mid_checkpoints'\n",
            "[rank0]:[W1016 15:22:44.631090689 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "E1016 15:22:45.835000 7555 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 7567) of binary: /content/nanochat/.venv/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanochat/.venv/bin/torchrun\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 901, in main\n",
            "    run(args)\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 892, in run\n",
            "    elastic_launch(\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 143, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 277, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "scripts.chat_eval FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2025-10-16_15:22:45\n",
            "  host      : 26106bffa2bf\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : 1 (pid: 7567)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n",
            "Overriding: run = dummy\n",
            "2025-10-16 15:22:55,588 - nanochat.common - \u001b[32m\u001b[1mINFO\u001b[0m - Distributed world size: 1\n",
            "[rank0]: Traceback (most recent call last):\n",
            "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "[rank0]:     return _run_code(code, main_globals, None,\n",
            "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "[rank0]:     exec(code, run_globals)\n",
            "[rank0]:   File \"/content/nanochat/scripts/chat_sft.py\", line 70, in <module>\n",
            "[rank0]:     model, tokenizer, meta = load_model(source, device, phase=\"train\", model_tag=model_tag, step=step)\n",
            "[rank0]:   File \"/content/nanochat/nanochat/checkpoint_manager.py\", line 146, in load_model\n",
            "[rank0]:     return load_model_from_dir(checkpoints_dir, *args, **kwargs)\n",
            "[rank0]:   File \"/content/nanochat/nanochat/checkpoint_manager.py\", line 125, in load_model_from_dir\n",
            "[rank0]:     model_tag = find_largest_model(checkpoints_dir)\n",
            "[rank0]:   File \"/content/nanochat/nanochat/checkpoint_manager.py\", line 93, in find_largest_model\n",
            "[rank0]:     model_tags = [f for f in os.listdir(checkpoint_dir) if os.path.isdir(os.path.join(checkpoint_dir, f))]\n",
            "[rank0]: FileNotFoundError: [Errno 2] No such file or directory: '/root/.cache/nanochat/mid_checkpoints'\n",
            "[rank0]:[W1016 15:22:57.343406584 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "E1016 15:22:59.242000 7634 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 7642) of binary: /content/nanochat/.venv/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanochat/.venv/bin/torchrun\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 901, in main\n",
            "    run(args)\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 892, in run\n",
            "    elastic_launch(\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 143, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 277, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "scripts.chat_sft FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2025-10-16_15:22:59\n",
            "  host      : 26106bffa2bf\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : 1 (pid: 7642)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n",
            "2025-10-16 15:23:07,946 - nanochat.common - \u001b[32m\u001b[1mINFO\u001b[0m - Distributed world size: 1\n",
            "[rank0]: Traceback (most recent call last):\n",
            "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "[rank0]:     return _run_code(code, main_globals, None,\n",
            "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "[rank0]:     exec(code, run_globals)\n",
            "[rank0]:   File \"/content/nanochat/scripts/chat_eval.py\", line 200, in <module>\n",
            "[rank0]:     model, tokenizer, meta = load_model(args.source, device, phase=\"eval\", model_tag=args.model_tag, step=args.step)\n",
            "[rank0]:   File \"/content/nanochat/nanochat/checkpoint_manager.py\", line 146, in load_model\n",
            "[rank0]:     return load_model_from_dir(checkpoints_dir, *args, **kwargs)\n",
            "[rank0]:   File \"/content/nanochat/nanochat/checkpoint_manager.py\", line 125, in load_model_from_dir\n",
            "[rank0]:     model_tag = find_largest_model(checkpoints_dir)\n",
            "[rank0]:   File \"/content/nanochat/nanochat/checkpoint_manager.py\", line 93, in find_largest_model\n",
            "[rank0]:     model_tags = [f for f in os.listdir(checkpoint_dir) if os.path.isdir(os.path.join(checkpoint_dir, f))]\n",
            "[rank0]: FileNotFoundError: [Errno 2] No such file or directory: '/root/.cache/nanochat/chatsft_checkpoints'\n",
            "[rank0]:[W1016 15:23:10.814137832 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "E1016 15:23:12.050000 7717 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 7729) of binary: /content/nanochat/.venv/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanochat/.venv/bin/torchrun\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 901, in main\n",
            "    run(args)\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 892, in run\n",
            "    elastic_launch(\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 143, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/content/nanochat/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 277, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "scripts.chat_eval FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2025-10-16_15:23:12\n",
            "  host      : 26106bffa2bf\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : 1 (pid: 7729)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n",
            "Generating report to /root/.cache/nanochat/report/report.md\n",
            "Warning: /root/.cache/nanochat/report/base-model-training.md does not exist, skipping\n",
            "Warning: /root/.cache/nanochat/report/base-model-loss.md does not exist, skipping\n",
            "Warning: /root/.cache/nanochat/report/base-model-evaluation.md does not exist, skipping\n",
            "Warning: /root/.cache/nanochat/report/midtraining.md does not exist, skipping\n",
            "Warning: /root/.cache/nanochat/report/chat-evaluation-mid.md does not exist, skipping\n",
            "Warning: /root/.cache/nanochat/report/chat-sft.md does not exist, skipping\n",
            "Warning: /root/.cache/nanochat/report/chat-evaluation-sft.md does not exist, skipping\n",
            "Warning: /root/.cache/nanochat/report/chat-rl.md does not exist, skipping\n",
            "Warning: /root/.cache/nanochat/report/chat-evaluation-rl.md does not exist, skipping\n",
            "Copying report.md to current directory for convenience\n"
          ]
        }
      ],
      "source": [
        "!bash ./speedrun.sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qbtbBb5VpSo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m scripts.chat_web"
      ],
      "metadata": {
        "id": "AmHFihmSjNkO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}